# -*- coding: utf-8 -*-
"""Sentiment Analysis

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_eQs7SKENRfTdu9k59zNtKjd-3zEKV7h
"""

import matplotlib.pyplot as plt
import seaborn as sns
import string
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import tensorflow as tf
import re
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from nltk.tokenize import word_tokenize
import keras
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score
import math
import nltk
from nltk.corpus import RegexpTokenizer as regextoken

nltk.download('wordnet')
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

df = pd.read_csv("/content/drive/MyDrive/Tweets.csv")
df.head()

df.columns

tweet_df = df[['text','airline_sentiment']]
print(tweet_df.shape)
tweet_df.head(5)

tweet_df = tweet_df[tweet_df['airline_sentiment'] != 'neutral']
tweet_df.head(10)

tweet_df["airline_sentiment"].value_counts()

# convert airline_seentiment to numeric
y = tweet_df.airline_sentiment.factorize()
y

# Preprocess the text with function processtext()
stop_words = stopwords.words("english")
stop_words.extend(["virginamerica", "VirginAmerica", "americanair", "usairways", "jetblue", "southwestair", "@"])
punctuations = list(string.punctuation)
lemma = WordNetLemmatizer() # for Lemmatisation

def processtext(text):
    # text = [word for word in text.split() if word not in (stop_words)]
    text=re.sub("[^a-zA-Z]"," ",text) # Filter to allow only alphabets in text
    text=text.lower() # Convert the text to lowercase to maintain consistency
    tokens=word_tokenize(text) # Tokenize the text
    tokens=[token for token in tokens if token not in stop_words] 
    tokens=[lemma.lemmatize(token) for token in tokens if token not in punctuations] # Lemmatisation of tokens
    tokens=[token for token in tokens if token not in stop_words] 
    text=" ".join(tokens)
    return text

tweet_df["processed_text"] = tweet_df.text.apply(lambda x: processtext(x))

tweet_df.head()

tweet = tweet_df.text.values
tokenizer = Tokenizer(num_words=5000)
tokenizer.fit_on_texts(tweet)
vocab_size = len(tokenizer.word_index) + 1
encoded_docs = tokenizer.texts_to_sequences(tweet)
X = pad_sequences(encoded_docs, maxlen=200)

# Build the model
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM,Dense, Dropout, SpatialDropout1D
from tensorflow.keras.layers import Embedding

embedding_vector_length = 32
model = Sequential() 
model.add(Embedding(vocab_size, embedding_vector_length, input_length=200) )
model.add(SpatialDropout1D(0.25))
model.add(LSTM(50, dropout=0.5, recurrent_dropout=0.5))
model.add(Dropout(0.2))
model.add(Dense(1, activation='sigmoid')) 
model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])  
print(model.summary())




history = model.fit(X, y[0],validation_split=0.2, epochs=5, batch_size=32)

# reviews on which we need to predict
sentence = ["The movie was very touching and heart whelming", 
            "I have never seen a terrible movie like this", 
            "the movie plot is terrible but it had good acting"]
# convert to a sequence
sequences = tokenizer.texts_to_sequences(sentence)
# pad the sequence
padded = pad_sequences(sequences, padding='post', maxlen=200)
# Get labels based on probability 1 if p>= 0.5 else 0
prediction = model.predict(padded)
pred_labels = []
for i in prediction:
    if i >= 0.5:
        pred_labels.append(1)
    else:
        pred_labels.append(0)
for i in range(len(sentence)):
    print(sentence[i])
    if pred_labels[i] == 1:
        s = 'Positive'
    else:
        s = 'Negative'
    print("Predicted sentiment : ",s)

model.save("model.h5")
import json

config = model.to_json()
config = json.loads(config)
with open('model.json', 'w') as f:
    json.dump(config, f)

plt.plot(history.history['accuracy'], label='acc')
plt.plot(history.history['val_accuracy'], label='val_acc')
plt.legend()
plt.show()
plt.savefig("Accuracy plot.jpg")

def predict_sentiment(text):
    tw = tokenizer.texts_to_sequences([text])
    tw = pad_sequences(tw,maxlen=200)
    prediction = int(model.predict(tw).round().item())
    print("Predicted label: ", y[1][prediction])

test_sentence1 = "I loved my journey on this flight."
predict_sentiment(test_sentence1)

test_sentence2 = "This is the worst flight experience of my life!"
predict_sentiment(test_sentence2)

